<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Latest News | ML for Data Management</title>
    <link>https://MLxDB.github.io/post/</link>
      <atom:link href="https://MLxDB.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Latest News</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 02 May 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://MLxDB.github.io/media/icon_hu977496d800f0d840e643bbe9854d61d3_17236_512x512_fill_lanczos_center_3.png</url>
      <title>Latest News</title>
      <link>https://MLxDB.github.io/post/</link>
    </image>
    
    <item>
      <title>Tutorial on ML4DB accepted at SIGMOD 2024</title>
      <link>https://MLxDB.github.io/post/24-05-02-sigmod-24-tutorial/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/24-05-02-sigmod-24-tutorial/</guid>
      <description>&lt;p&gt;Congratulations to Yang Jingyi and Zhao Yue for publishing the tutorial &amp;ldquo;Machine Learning for Databases: Foundations, Paradigms, and Open problems&amp;rdquo; at SIGMOD 2024.&lt;/p&gt;
&lt;p&gt;This tutorial delves into the burgeoning field of Machine Learning for Databases (ML4DB),  highlighting its recent progress and the challenges impeding its integration into industrial-grade database management systems. We systematically explore three key themes: the exploration of foundations in ML4DB and their potential for diverse applications, the two paradigms in ML4DB, i.e., using machine learning as a replacement versus enhancement of traditional database components, and the critical open challenges such as improving model efficiency and addressing data shifts. Through an in-depth analysis, including a survey of recent works in major database conferences, this tutorial encapsulates the current state of ML4DB, as well as charts a roadmap for its future development and wider adoption in practical database environments.&lt;/p&gt;
&lt;p&gt;You may access the tutorial slides &lt;a href=&#34;https://github.com/MLxDB/MLxDB.github.io/blob/master/publication/sigmod24-tutorial/ml4db-foundations-paradigms-open-problems.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Evaluation Paper on Query Plan Representation accepted at VLDB 2024</title>
      <link>https://MLxDB.github.io/post/24-05-01-vldb-24-evaluation/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/24-05-01-vldb-24-evaluation/</guid>
      <description>&lt;p&gt;Congratulations to Zhao Yue and Li Zhaodonghui for publishing the paper &amp;ldquo;A Comparative Study and Component Analysis of Query Plan Representation Techniques in ML4DB Studies&amp;rdquo; at VLDB 2024.&lt;/p&gt;
&lt;p&gt;Query plan is widely used as input in machine learning for databases (ML4DB) research, with query plan representation as a critical step. However, existing studies typically focus on one task, and propose a novel design to represent query plans along with a ML4DB framework, without comparing with other representation methods designed for a different task. This raises a critical question: How do we select a query plan representation method in a ML4DB system?&lt;/p&gt;
&lt;p&gt;To address this question, we perform a comparative study on ten representation methods on three distinct ML4DB tasks: cost estimation, index selection and query optimization. Our extensive experiments not only verify the interchangeability of representation methods across different tasks, but also identify consistently high-performing models. Further, we dissect the query plan representation into two core components: feature encoding and tree model, and evaluate the impact of design choices for each in different scenarios. Our results show that the findings for tasks optimizing absolute errors are different from findings for tasks optimizing relative errors. Some findings challenge widely-held assumptions, i.e., one finding shows that tree models do not significantly impact cost estimation results, but only play a significant role to optimize relative performance. Practical guidelines and future directions are provided based on the findings of the study.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper on learned R-tree packing accepted at SIGMOD 2024</title>
      <link>https://MLxDB.github.io/post/23-08-26-sigmod-24-platon/</link>
      <pubDate>Sat, 26 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/23-08-26-sigmod-24-platon/</guid>
      <description>&lt;p&gt;Congratulations to Jingyi for publishing the paper &amp;ldquo;PLATON: Top-down R-tree Packing with Learned Partition Policy&amp;rdquo; at SIGMOD 2024.&lt;/p&gt;
&lt;p&gt;The exponential growth of spatial data poses new challenges to the performance of spatial databases. Spatial indexes like R-tree greatly accelerate the query performance and can be effectively constructed through packing, i.e., loading all data into the index at once. However, existing R-tree packing methods rely on a set of fixed heuristic rules, which may not be suitable for different data distributions and workload patterns. To address the limitations of existing R-tree packing methods, we propose PLATON, a top-down R-tree packing method with learned partition policy that explicitly optimizes the query performance with regard to the given data and workload instance. We develop a learned partition policy based on Monte Carlo Tree Search and carefully make design choices for the MCTS exploration strategy and simulation strategy to improve algorithm convergence. We propose a divide and conquer strategy and two optimization techniques, early termination and level-wise sampling, to drastically reduce the MCTS algorithmâ€™s time complexity and make it a linear-time algorithm. Experiments on both synthetic and real-world datasets demonstrate the superior performance of PLATON over existing R-tree variants and recently proposed learned/workload-aware spatial indexes.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper on multi-query optimization accepted at SIGMOD 2024</title>
      <link>https://MLxDB.github.io/post/23-08-26-sigmod-24-lemo/</link>
      <pubDate>Sat, 26 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/23-08-26-sigmod-24-lemo/</guid>
      <description>&lt;p&gt;Congratulations to Songsong, Yile for publishing the paper &amp;ldquo;Lemo: A Cache-Enhanced Learned Optimizer for Concurrent Queries&amp;rdquo; at SIGMOD 2024.&lt;/p&gt;
&lt;p&gt;With the expansion of modern database services, multi-user access has become a crucial feature in various practical application scenarios, including enterprise applications and e-commerce platforms. However, if multiple users submit queries within a short time frame, it can result in potential issues such as redundant computation and query concurrency. Unfortunately, most existing multi-query optimization methods, which aim to enhance query processing efficiency, have not adequately addressed these two problems, especially in the setting where multiple queries are being executed concurrently. To this end, we propose a novel method named Lemo for the multi-query optimization problem. Specifically, we propose a novel value network to predict latencies of concurrent queries as the foundation model for query plan generation. Furthermore, we introduce a shared buffer manager component to cache the intermediate results of sub-queries. The shared buffer manager applies a novel replacement policy to maintain the cached buffer with the objective of maximizing the opportunity for the reuse of the cached sub-queries. Based on the shared buffer, our proposed value network can incorporate the cached results into cost estimation to further guide Lemo in generating query plans, thus avoiding redundant computation. Lemo has been integrated into PostgreSQL and experiments conducted on real datasets with PostgreSQL show that it outperforms all the baselines in efficiency.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper on selectivity estimation for set-valued data accepted at SIGMOD 2024</title>
      <link>https://MLxDB.github.io/post/23-08-26-sigmod-24-set/</link>
      <pubDate>Sat, 26 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/23-08-26-sigmod-24-set/</guid>
      <description>&lt;p&gt;Congratulations to Zizhong for publishing the paper &amp;ldquo;Selectivity Estimation for Queries Containing Predicates over Set-Valued Attributes&amp;rdquo; at SIGMOD 2024.&lt;/p&gt;
&lt;p&gt;Selectivity estimation aims to estimate the size of query results size accurately and efficiently. Despite being a well-researched area for decades, most existing estimators are designed to handle comparison predicates over numeric and categorical data. Nevertheless, Set-valued data are ubiquitous in many applications such as information retrieval and recommendation systems. However, these estimators may not be effective for handling predicates over set-valued data. In this work, we presents novel techniques for selectivity estimation on queries involving predicates over set-valued attributes. We first propose the set-valued column factorization problem, whereby each each set-valued column is converted to multiple numeric subcolumns, and set containment predicates are converted to numeric comparison predicates. This enables us to leverage any existing estimator to perform selectivity estimation. We then develop two methods for column factorization and query conversion, namely ST and ST-hist. We integrate ST and ST-hist into three estimators, Postgres, Neurocard, and DeepDB. We then conduct a comprehensive empirical analysis by comparing our approach against three baselines across three different datasets. The experimental results demonstrate that our methods exhibit superior estimation accuracy while maintaining high efficiency compared to the baseline techniques.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper on Learned Space-Filling Curve accepted at VLDB 2023</title>
      <link>https://MLxDB.github.io/post/23-05-01-vldb-23/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/23-05-01-vldb-23/</guid>
      <description>&lt;p&gt;Congratulations to Jiangneng for publishing the paper &amp;ldquo;Towards Designing and Learning Piecewise Space-Filling Curves&amp;rdquo; at VLDB 2023.&lt;/p&gt;
&lt;p&gt;To index multi-dimensional data, space-filling curves (SFCs) have
been used to map the data to one dimension, and then a one dimensional indexing method such as the B-tree is used to index the mapped data. The existing SFCs all adopt a single mapping scheme for the whole data space. However, a single mapping scheme often does not performwell on all the data space. In this paper,we propose a new type of SFC called piecewise SFCs, which adopts different mapping schemes for different data subspaces. Specifically, we propose a data structure called Bit Merging tree (BMTree), which can generate data subspaces and their SFCs simultaneously and achieve desirable properties of the SFC for the whole data space. Furthermore, we develop a reinforcement learning based solution to build the BMTree, aiming to achieve excellent query performance. Extensive experiments show that our proposed method outperforms existing SFCs in terms of query performance.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper on Learned R-tree accepted at SIGMOD 2023</title>
      <link>https://MLxDB.github.io/post/22-11-15-sigmod-23/</link>
      <pubDate>Tue, 15 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/22-11-15-sigmod-23/</guid>
      <description>&lt;p&gt;Congratulations to Gu Tu for publishing the paper &amp;ldquo;Effectively Learning Spatial Indexes with a Support for Updates&amp;rdquo; at SIGMOD 2023.&lt;/p&gt;
&lt;p&gt;Learned indices have been proposed to replace classic index structures like B-Tree with machine learning (ML) models. They require to replace both the indices and query processing algorithms currently deployed by the databases, and such a radical departure is likely to encounter challenges and obstacles. In contrast, we propose a fundamentally different way of using ML techniques to build a better R-Tree without the need to change the structure or query processing algorithms of traditional R-Tree. Specifically, we develop reinforcement learning (RL) based models to decide how to choose a subtree for insertion and how to split a node when building and updating an R-Tree, instead of relying on hand-crafted heuristic rules currently used by the R-Tree and its variants. Experiments on real and synthetic datasets with up to more than 100 million spatial objects show that our RL based index outperforms the R-Tree and its variants in terms of query processing time.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper on Learned Index Selection accepted at VLDB 2023</title>
      <link>https://MLxDB.github.io/post/22-08-15-vldb-23/</link>
      <pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/22-08-15-vldb-23/</guid>
      <description>&lt;p&gt;Congratulations to Shi Jiachen for publishing the paper &amp;ldquo;Learned Index Benefits: Machine Learning Based Index Performance Estimation&amp;rdquo; at VLDB 2023.&lt;/p&gt;
&lt;p&gt;Index selection remains one of the most challenging problems in
relational database management systems. To find an optimum index configuration for a workload, accurately and efficiently quantifying
the benefits of each candidate index configuration is indispensable. As
materializing each index configuration candidate and physically executing
queries are infeasible, most of index tuners rely on the cost
estimations from optimizer with &amp;ldquo;what-if&amp;rdquo; API. However, &amp;ldquo;what-if&amp;rdquo;
based index benefit estimations have the following two limitations.
Firstly, they generate significant errors, which compromise index
recommendation quality. Secondly, generating query plans and
benefit estimations for each candidate index configuration takes
a considerable amount of time. To address the two challenges in
index selection, we propose an effective end-to-end machine learning
based index benefit estimator. In particular, we propose novel feature
extraction and encoding techniques that do not rely on &amp;ldquo;what-if&amp;rdquo;
call to generate query plan for each index configuration candidate.
In addition, we design an attention mechanism to address index
interaction issue and aggregate the impacts of different query operations.
Finally, we leverage transfer learning technique to improve
the estimatorâ€™s learning ability for adaption to new database. Comprehensive experiments are conducted on different workloads, and
extensive experimental results show that our proposed method
outperforms &amp;ldquo;what-if&amp;rdquo; based index benefit estimations in terms of
accuracy and efficiency. In addition, integrating our method into
existing index selection algorithms can significantly improve index
recommendation quality.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper on database generation accepted at SIGMOD 2022</title>
      <link>https://MLxDB.github.io/post/22-03-09-sigmod-22/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/22-03-09-sigmod-22/</guid>
      <description>&lt;p&gt;Congratulations to Yang Jingyi and Wu Peizhi for publishing the paper &amp;ldquo;SAM: Database Generation from Query Workloads with Supervised Autoregressive Models&amp;rdquo; at SIGMOD 2022.&lt;/p&gt;
&lt;p&gt;With the prevalence of cloud databases, database users are increasingly reliant on the cloud database providers to manage their data. It becomes a challenge for cloud providers to benchmark different DBMS for a specific database instance without having access to the underlying data. One viable solution is to leverage a query workload, which contains a set of queries and the corresponding cardinalities, to generate a synthetic database with similar query performance. Existing methods for database generation with cardinality constraints, however, can only handle very small query workloads due to their high complexity and encounter challenges when handling join queries. In this work, we propose SAM, a supervised deep autoregressive model-based method for database generation from query workloads. First, SAM is able to process large-scale query workloads efficiently as its complexity is linear in the size of the query workload, the number of attributes and the attribute domain size. Second, we develop algorithms to obtain unbiased samples of base relations from the deep autoregressive model and assign join keys in a way that accurately recovers the full outer join of the target database. Comprehensive experiments on real-world datasets demonstrate that SAM is able to efficiently generate a high-fidelity database that not only satisfies the input cardinality constraints, but also is close to the target database.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper on Query Plan Representation accepted at VLDB 2022</title>
      <link>https://MLxDB.github.io/post/22-03-01-vldb-22/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/22-03-01-vldb-22/</guid>
      <description>&lt;p&gt;Congratulations to Zhao Yue and Shi Jiachen for publishing the paper &amp;ldquo;QueryFormer: A Tree Transformer Model for Query Plan
Representation&amp;rdquo; at VLDB 2022.&lt;/p&gt;
&lt;p&gt;Machine learning has become a prominent method in many database optimization problems such as cost estimation, index selection and query optimization. Translating query execution plans into their vectorized representations is non-trivial. Recently, several query plan representation methods have been proposed. However, they have two limitations. First, they do not fully utilize readily available database statistics in the representation, which characterizes the data distribution. Second, they typically have difficulty in modeling long paths of information flow in a query plan, and capturing parent-children dependency between operators.&lt;/p&gt;
&lt;p&gt;To tackle these limitations, we propose QueryFormer, a learning-based
query plan representation model with a tree-structured Transformer architecture. In particular, we propose a novel scheme to integrate histograms obtained from database systems into query plan encoding. In addition, to effectively capture the information flow following the tree structure of a query plan, we develop a tree-structured model with the attention mechanism. We integrate QueryFormer into four machine learning models, each for a database optimization task, and experimental results show that QueryFormer is able to improve performance of these models significantly.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
