<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Latest News | ML for Database</title>
    <link>https://MLxDB.github.io/post/</link>
      <atom:link href="https://MLxDB.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Latest News</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 09 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://MLxDB.github.io/media/icon_hu977496d800f0d840e643bbe9854d61d3_17236_512x512_fill_lanczos_center_3.png</url>
      <title>Latest News</title>
      <link>https://MLxDB.github.io/post/</link>
    </image>
    
    <item>
      <title>Paper on database generation accepted at SIGMOD 2022</title>
      <link>https://MLxDB.github.io/post/22-03-09-sigmod-22/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/22-03-09-sigmod-22/</guid>
      <description>&lt;p&gt;Congratulations to Yang Jingyi and Wu Peizhi for publishing the paper &amp;ldquo;SAM: Database Generation from Query Workloads with Supervised Autoregressive Models&amp;rdquo; at SIGMOD 2022.&lt;/p&gt;
&lt;p&gt;With the prevalence of cloud databases, database users are increasingly reliant on the cloud database providers to manage their data. It becomes a challenge for cloud providers to benchmark different DBMS for a specific database instance without having access to the underlying data. One viable solution is to leverage a query workload, which contains a set of queries and the corresponding cardinalities, to generate a synthetic database with similar query performance. Existing methods for database generation with cardinality constraints, however, can only handle very small query workloads due to their high complexity and encounter challenges when handling join queries. In this work, we propose SAM, a supervised deep autoregressive model-based method for database generation from query workloads. First, SAM is able to process large-scale query workloads efficiently as its complexity is linear in the size of the query workload, the number of attributes and the attribute domain size. Second, we develop algorithms to obtain unbiased samples of base relations from the deep autoregressive model and assign join keys in a way that accurately recovers the full outer join of the target database. Comprehensive experiments on real-world datasets demonstrate that SAM is able to efficiently generate a high-fidelity database that not only satisfies the input cardinality constraints, but also is close to the target database.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper on Query Plan Representation accepted at VLDB 2022</title>
      <link>https://MLxDB.github.io/post/22-03-01-vldb-22/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://MLxDB.github.io/post/22-03-01-vldb-22/</guid>
      <description>&lt;p&gt;Congratulations to Zhao Yue and Shi Jiachen for publishing the paper &amp;ldquo;QueryFormer: A Tree Transformer Model for Query Plan
Representation&amp;rdquo; at VLDB 2022.&lt;/p&gt;
&lt;p&gt;Machine learning has become a prominent method in many database optimization problems such as cost estimation, index selection and query optimization. Translating query execution plans into their vectorized representations is non-trivial. Recently, several query plan representation methods have been proposed. However, they have two limitations. First, they do not fully utilize readily available database statistics in the representation, which characterizes the data distribution. Second, they typically have difficulty in modeling long paths of information flow in a query plan, and capturing parent-children dependency between operators.&lt;/p&gt;
&lt;p&gt;To tackle these limitations, we propose QueryFormer, a learning-based
query plan representation model with a tree-structured Transformer architecture. In particular, we propose a novel scheme to integrate histograms obtained from database systems into query plan encoding. In addition, to effectively capture the information flow following the tree structure of a query plan, we develop a tree-structured model with the attention mechanism. We integrate QueryFormer into four machine learning models, each for a database optimization task, and experimental results show that QueryFormer is able to improve performance of these models significantly.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
